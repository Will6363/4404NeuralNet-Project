""" Neural Network.
A 2-Hidden Layers Fully Connected Neural Network (a.k.a Multilayer Perceptron)
implementation with TensorFlow. This example is using the MNIST database
of handwritten digits (http://yann.lecun.com/exdb/mnist/).
Links:
    [MNIST Dataset](http://yann.lecun.com/exdb/mnist/).
Author: Aymeric Damien
Project: https://github.com/aymericdamien/TensorFlow-Examples/

Edited by: William Zhen   SID: 214305171    (in Collaboration with Aliza Hasan)
For EECS 4404 F 2019
"""


from __future__ import print_function
%tensorflow_version 1.x
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

def data_segmentation(data_path, target_path, task):
  # task = 0 >> select the name ID targets for face recognition task
  # task = 1 >> select the gender ID targets for gender recognition task
  data = np.load(data_path)/255
  data = np.reshape(data, [-1, 32*32])
  target = np.load(target_path)
  np.random.seed(45689)
  rnd_idx = np.arange(np.shape(data)[0])
  np.random.shuffle(rnd_idx)
  trBatch = int(0.8*len(rnd_idx))
  validBatch = int(0.1*len(rnd_idx))
  trainData, validData, testData = data[rnd_idx[1:trBatch],:], \
  data[rnd_idx[trBatch+1:trBatch + validBatch],:],\
  data[rnd_idx[trBatch + validBatch+1:-1],:]
  trainTarget, validTarget, testTarget = target[rnd_idx[1:trBatch], task], \
  target[rnd_idx[trBatch+1:trBatch + validBatch], task],\
  target[rnd_idx[trBatch + validBatch + 1:-1], task]
  return trainData, validData, testData, trainTarget, validTarget, testTarget

# Get Data Segments
trainData, validData, testData, trainTarget, validTarget, testTarget = data_segmentation("data.npy", "target.npy", 0)

# Records for loss and accuracy
steps_list = []
train_loss = []
train_acc = []
valid_loss = []
valid_acc = []
test_loss = []
test_acc = []

# Parameters
learning_rate = 0.0005
num_steps = 100
display_step = 25

# Network Parameters
n_hidden_1 = 384  # 1st layer number of neurons
n_hidden_2 = 192  # 2nd layer number of neurons
num_input = 32*32 # data input (img shape: 32*32)
class_celeb = 6 # total classes (0-5 digits)
class_gender = 2

# tf Graph input for X (image) Y (labels)
X = tf.placeholder("float", [None, num_input])
Y = tf.placeholder("int64", [None])
prob = tf.placeholder_with_default(0.0, shape=())       # prob of dropout rate (default 0.0)

# build a layer with input X and hidden_units neurons
def building_block_normal(X, hidden_units):
    input_units = X.get_shape().as_list()[1]
    shape = [input_units, hidden_units]
    W = tf.Variable(tf.contrib.layers.xavier_initializer()(shape))
    b = tf.Variable(0, dtype=tf.float32)
    S = tf.add(tf.matmul(X, W), b)
    return S
# build a Convolutional block from inputs
def building_block_conv(X, output_num, k_size, stride):
    return tf.layers.conv2d(X, output_num, k_size, strides=stride, kernel_initializer=tf.contrib.layers.xavier_initializer())

# Create pool2d from inputs
def conv_pool2d(X, p_size, stride):
    return tf.layers.max_pooling2d(X, p_size, stride)

# Create activation function
def activation(input_vector):
    return tf.nn.relu(input_vector)

# Create model
def neural_net(x):
  x = tf.reshape(x, shape=[-1, 32, 32, 1])

  c_layer = building_block_conv(x, 32, (5, 5), (1,1))
  #print(c_layer.shape)
  c_pool = conv_pool2d(c_layer, (3, 3), (2, 2))
  c_flat = tf.layers.flatten(c_pool)
      
  layer_1 = building_block_normal(c_flat, n_hidden_1)           # Hidden fully connected layer
  layer_1 = tf.nn.dropout(layer_1, rate=prob)                   # drop out with rate prob
  layer_1 = activation(layer_1)                                 # Hidden layer 1 activation

  layer_2 = building_block_normal(layer_1, n_hidden_2)
  layer_2 = tf.nn.dropout(layer_2, rate=prob)
  layer_2 = activation(layer_2)

  out_layer = building_block_normal(layer_2, class_celeb)           # Output fully connected layer with a neuron for each class
  #out_layer = tf.nn.softmax(out_layer)                    # Apply soft max afterwards to avoid feeding spare_softmax_cross_entroy_with_logits a scaled logits (For 1.3 pretend this line is used to give probabilities)
  return out_layer

# Construct model
logits = neural_net(X)
prediction = tf.nn.softmax(logits)                          # Applys soft max to get the probability of each celebrity

# Define loss and optimizer
loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(
    logits=logits, labels=Y))
optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)
train_op = optimizer.minimize(loss_op)

# Evaluate model
correct_pred = tf.equal(tf.argmax(prediction, 1), Y)
accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))

# Initialize the variables (i.e. assign their default value)
init = tf.global_variables_initializer()

# Start training
with tf.Session() as sess:

  # Run the initializer
  sess.run(init)
  # saver.save(sess, 'Neural Model') ???

  for step in range(1, num_steps+1):
    # Run optimization op (backprop)
    sess.run(train_op, feed_dict={X: trainData.eval(), Y: trainTarget, prob: 0.5})          # define dropout rate as prob
    steps_list.append(step)

    # Calculate batch loss and accuracy
    loss, acc = sess.run([loss_op, accuracy], feed_dict={X: trainData.eval(), Y: trainTarget}) 
    train_loss.append(loss)
    train_acc.append(acc)
    if step % display_step == 0 or step == 1:
      print("Step " + str(step) + ", Training Loss= " + \
          "{:.4f}".format(loss) + ", Training Accuracy= " + \
          "{:.3f}".format(acc)) 

    # Calculate validation loss and accuracy
    vloss, vacc = sess.run([loss_op, accuracy], feed_dict={X: validData.eval(), Y: validTarget})
    valid_loss.append(vloss)
    valid_acc.append(vacc)
    if step % display_step == 0 or step == 1:
      print ("Validation Loss= " + \
          "{:.4f}".format(vloss) + ", Validation Accuracy= " + \
          "{:.3f}".format(vacc))

    # Calculate test loss and accuracy
    tloss, tacc = sess.run([loss_op, accuracy], feed_dict={X: testData.eval(), Y: testTarget})
    test_loss.append(tloss)
    test_acc.append(tacc)
    if step % display_step == 0 or step == 1:
      print ("Testing Loss= " + \
          "{:.4f}".format(tloss) + ", Testing Accuracy= " + \
          "{:.3f}".format(tacc))

# Graphing and debugging
x_axis = steps_list
y1_axis = train_loss
y2_axis = valid_loss
y3_axis = test_loss
y4_axis = train_acc
y5_axis = valid_acc
y6_axis = test_acc

plt.figure()
plt.plot(x_axis,y1_axis)
plt.plot(x_axis,y2_axis)
plt.plot(x_axis,y3_axis)
plt.legend(['train_loss', 'valid_loss', 'test_loss'], loc='upper right') # loss
plt.show()
plt.plot(x_axis, y4_axis)
plt.plot(x_axis, y5_axis)
plt.plot(x_axis, y6_axis)
plt.legend(['train_acc', 'valid_acc', 'test_acc'], loc='lower right') # acc
plt.show()
